    
    
    other implementation I was working on that I would like to test out. 
    
    train_loader = DataLoader(dataset["train"], batch_size=batch_size, shuffle=True, num_workers=num_workers)
    test_loader = DataLoader(dataset["test"], batch_size=batch_size, shuffle=False, num_workers=num_workers)
    
    
    for epoch in range(num_epochs): 
        model.train()
        for batch in train_loader: 
            optimizer.zero_grad()
            inputs = {k: v.to('cuda') for k, v in batch.items()}
            outputs = model(**inputs)
            loss = criterion(outputs.logits, inputs['labels'].to('cuda'))
            loss.backward()
            optimizer.step()

        model.eval()
        with torch.no_grad(): 
            total_correct = 0 
            total_samples = 0
            for batch in test_loader: 
                inputs = {k: v.to('cuda') for k, v in batch.items()}
                outputs = model(**inputs)
                _, predicted = torch.max(outputs.logits, 1)
                total_correct += (predicted == inputs['labels'].to('cuda')).sum().item()
                total_samples += inputs['labels'].size(0)
            accuracy = total_correct / total_samples
            print(f"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.4f}")



'accuracy': 0.9284420289855072, 'precision': 0.9897774327122153, 'recall': 0.14285714285714285, 'f1': 0.13755619673891165   40 epochs 8 batch .001 lr

{'accuracy': 0.9208937198067633, 'precision': 0.8712176177236418, 'recall': 0.16528308059740557, 'f1': 0.16099920878815613}  40 epochs 8 batch .0001 lr

{'accuracy': 0.29257246376811596, 'precision': 0.7054182809459529, 'recall': 0.1358391405342625, 'f1': 0.06938331298886065}  40 epochs 8 batch .000001 lr

{'accuracy': 0.9284420289855072, 'precision': 0.9897774327122153, 'recall': 0.14285714285714285, 'f1': 0.13755619673891165} 40 epochs 8 batch .00001 lr

{'accuracy': 0.9109299516908212, 'precision': 0.8702550700129391, 'recall': 0.1775092833423304, 'f1': 0.16436684198681964} 80 epochs 8 batch .0001 lr

{'accuracy': 0.9251207729468599, 'precision': 0.8764440946259128, 'recall': 0.16003664261994732, 'f1': 0.1597986991096964} 100 epochs 8 batch and .0001 lr


{'accuracy': 0.9166666666666666, 'precision': 0.8743806574729822, 'recall': 0.17446074822921268, 'f1': 0.1666633451027415} 80 epochs 6 batch and .0001 lr


{'accuracy': 0.9181763285024155, 'precision': 0.5834529213455065, 'recall': 0.16093373247615775, 'f1': 0.15779432475580207} 80 epohs 4 batch and .0001 lr


{'accuracy': 0.9181763285024155, 'precision': 0.5834529213455065, 'recall': 0.16093373247615775, 'f1': 0.15779432475580207} 40 epohs 10 batch and .0001 lr

{'accuracy': 0.9257246376811594, 'precision': 0.7341940132781828, 'recall': 0.1561983281804649, 'f1': 0.15664207748590622} 80 epochs 12 batch and .0001 lr

{'accuracy': 0.9284420289855072, 'precision': 0.9897774327122153, 'recall': 0.14285714285714285, 'f1': 0.13755619673891165} 40 epochs 16 batch size .001 lr


{'accuracy': 0.9290458937198067, 'precision': 0.7522364332682997, 'recall': 0.15277813220788142, 'f1': 0.15440003755143764} 80 epochs 2 batch .001 lr 32 steps

